{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a40483-5344-4272-86e8-e3402bbd27b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: /Users/roshin/Documents/Slides/templates/lama/output/chunk_1.big\n",
      "Created: /Users/roshin/Documents/Slides/templates/lama/output/chunk_2.big\n",
      "Created: /Users/roshin/Documents/Slides/templates/lama/output/chunk_3.big\n",
      "Created: /Users/roshin/Documents/Slides/templates/lama/output/chunk_4.big\n",
      "Created: /Users/roshin/Documents/Slides/templates/lama/output/chunk_5.big\n",
      "Created: /Users/roshin/Documents/Slides/templates/lama/output/chunk_6.big\n",
      "Created: /Users/roshin/Documents/Slides/templates/lama/output/chunk_7.big\n",
      "Created: /Users/roshin/Documents/Slides/templates/lama/output/chunk_8.big\n",
      "Created: /Users/roshin/Documents/Slides/templates/lama/output/chunk_9.big\n",
      "Created: /Users/roshin/Documents/Slides/templates/lama/output/chunk_10.big\n",
      "Created: /Users/roshin/Documents/Slides/templates/lama/output/chunk_11.big\n",
      "Created: /Users/roshin/Documents/Slides/templates/lama/output/chunk_12.big\n",
      "Created: /Users/roshin/Documents/Slides/templates/lama/output/chunk_13.big\n",
      "File split successfully into 13 chunks!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def split_file(file_path, output_dir, chunk_size):\n",
    "    \"\"\"\n",
    "    Splits a file into smaller chunks.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file to be split.\n",
    "        output_dir (str): Directory where the chunks will be saved.\n",
    "        chunk_size (int): Size of each chunk in bytes.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the output directory exists\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # Open the file to read in binary mode\n",
    "        with open(file_path, 'rb') as file:\n",
    "            chunk_number = 1\n",
    "            while True:\n",
    "                # Read a chunk of the file\n",
    "                chunk = file.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                \n",
    "                # Write the chunk to a new file\n",
    "                chunk_file_path = os.path.join(output_dir, f\"chunk_{chunk_number}.big\")\n",
    "                with open(chunk_file_path, 'wb') as chunk_file:\n",
    "                    chunk_file.write(chunk)\n",
    "                \n",
    "                print(f\"Created: {chunk_file_path}\")\n",
    "                chunk_number += 1\n",
    "\n",
    "        print(f\"File split successfully into {chunk_number - 1} chunks!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during file splitting: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the file to be split and output directory\n",
    "    file_to_split = \"/Users/roshin/Documents/Slides/templates/lama/model-00001-of-00002.safetensors\"\n",
    "    output_directory = \"/Users/roshin/Documents/Slides/templates/lama/output\"\n",
    "    \n",
    "    # Define the chunk size in bytes (e.g., 10 MB = 10 * 1024 * 1024 bytes)\n",
    "    chunk_size_in_bytes = 400 * 1024 * 1024  # 10 MB\n",
    "    \n",
    "    # Call the function to split the file\n",
    "    split_file(file_to_split, output_directory, chunk_size_in_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adcfcae-12be-4eaf-96d1-f841c8e0054c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df1590bd-ecde-42ac-9406-b054ac8ead34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_1\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_2\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_3\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_4\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_5\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_6\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_7\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_8\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_9\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_10\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_11\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_12\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_13\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_14\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_15\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_16\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_17\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_18\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_19\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_20\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_21\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_22\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_23\n",
      "File merged successfully into: /Users/roshin/Documents/Slides/templates/mlx/model_merged new.safetensors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def merge_chunks(output_file_path, chunks_dir):\n",
    "    \"\"\"\n",
    "    Merges chunks back into the original file.\n",
    "    \n",
    "    Args:\n",
    "        output_file_path (str): Path where the merged file will be saved.\n",
    "        chunks_dir (str): Directory containing the chunk files to merge.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get a sorted list of all chunk files\n",
    "        chunk_files = sorted(\n",
    "            [f for f in os.listdir(chunks_dir) if f.startswith(\"chunk_\")],\n",
    "            key=lambda x: int(x.split('_')[-1])  # Sort by chunk number\n",
    "        )\n",
    "        \n",
    "        # Merge all chunks into the output file\n",
    "        with open(output_file_path, 'wb') as output_file:\n",
    "            for chunk_file in chunk_files:\n",
    "                chunk_file_path = os.path.join(chunks_dir, chunk_file)\n",
    "                with open(chunk_file_path, 'rb') as cf:\n",
    "                    output_file.write(cf.read())\n",
    "                print(f\"Merged: {chunk_file_path}\")\n",
    "\n",
    "        print(f\"File merged successfully into: {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during file merging: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the directory containing the chunks and the output file\n",
    "    chunks_directory = \"/Users/roshin/Documents/Slides/templates/mlx/output\"\n",
    "    merged_file_path = \"/Users/roshin/Documents/Slides/templates/mlx/model_merged new.safetensors\"\n",
    "    \n",
    "    # Call the function to merge chunks\n",
    "    merge_chunks(merged_file_path, chunks_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e32f63-01ad-4eb3-b5ed-c122d409feb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc4d8c-d75e-4306-a0d6-facfa0643271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx import MLXModel, MLXTokenizer\n",
    "\n",
    "def load_mlx_llm_model(model_path):\n",
    "    \"\"\"\n",
    "    Loads a local MLX LLM model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the directory containing the MLX model files.\n",
    "        \n",
    "    Returns:\n",
    "        model, tokenizer: Loaded MLX model and tokenizer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the MLX model and tokenizer\n",
    "        model = MLXModel.from_pretrained(model_path)\n",
    "        tokenizer = MLXTokenizer.from_pretrained(model_path)\n",
    "        print(\"MLX LLM model and tokenizer loaded successfully!\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MLX LLM model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def generate_response(prompt, model, tokenizer, max_length=100):\n",
    "    \"\"\"\n",
    "    Generates a response to a given prompt using the MLX LLM model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt text.\n",
    "        model (MLXModel): Loaded MLX model.\n",
    "        tokenizer (MLXTokenizer): Tokenizer for the MLX model.\n",
    "        max_length (int): Maximum length of the generated response.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated response text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Tokenize the input prompt\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate a response using the model\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        # Decode the output tokens\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the local MLX model directory\n",
    "    model_directory = \"./mlx_llm_model\"  # Replace with the actual path to your model\n",
    "    \n",
    "    # Load the MLX LLM model and tokenizer\n",
    "    mlx_model, mlx_tokenizer = load_mlx_llm_model(model_directory)\n",
    "    \n",
    "    if mlx_model and mlx_tokenizer:\n",
    "        # Example prompt\n",
    "        example_prompt = \"What are the applications of large language models?\"\n",
    "        \n",
    "        # Generate a response\n",
    "        response = generate_response(example_prompt, mlx_model, mlx_tokenizer)\n",
    "        \n",
    "        if response:\n",
    "            print(\"Generated Response:\")\n",
    "            print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c300fd-2066-4537-8ea6-ca3275deeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "model_path = \"mlx-community/Mistral-7B-Instruct-v0.2-4bit\"\n",
    "prompt = prompt_builder(\"Great content, thank you!\")\n",
    "max_tokens = 140\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.2-4bit\")\n",
    "response = generate(model, tokenizer, prompt=prompt, max_tokens = max_tokens,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e2cbc-1650-4316-accf-10660b7f099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Step 1: Web Crawler to Scrape Links and Sub-links\n",
    "def crawl_website(base_url, max_depth=2):\n",
    "    visited_urls = set()\n",
    "    content_data = []\n",
    "\n",
    "    def scrape_page(url, depth):\n",
    "        if depth > max_depth or url in visited_urls:\n",
    "            return\n",
    "        \n",
    "        print(f\"Scraping: {url}\")\n",
    "        visited_urls.add(url)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                return\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract text content\n",
    "            page_text = \" \".join([p.get_text() for p in soup.find_all('p')])\n",
    "            if page_text.strip():\n",
    "                content_data.append({\"url\": url, \"content\": page_text.strip()})\n",
    "            \n",
    "            # Find all sub-links on the page\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_link = urljoin(url, link['href'])\n",
    "                # Only follow links within the same domain\n",
    "                if base_url in absolute_link and absolute_link not in visited_urls:\n",
    "                    scrape_page(absolute_link, depth + 1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {url}: {e}\")\n",
    "\n",
    "    # Start scraping from the base URL\n",
    "    scrape_page(base_url, depth=0)\n",
    "    return content_data\n",
    "\n",
    "# Step 2: Save Data in LLM Training-Friendly Format\n",
    "def save_data_to_llm_format(data, output_file=\"website_data.jsonl\"):\n",
    "    \"\"\"\n",
    "    Save data in JSONL format where each line represents a training example:\n",
    "    {\n",
    "        \"url\": \"page_url\",\n",
    "        \"content\": \"text_content\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for entry in data:\n",
    "            json.dump(entry, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://example.com\"  # Replace with the website you want to scrape\n",
    "    max_depth = 2  # Define the depth of crawling\n",
    "\n",
    "    # Crawl the website\n",
    "    crawled_data = crawl_website(base_url, max_depth=max_depth)\n",
    "    print(f\"Scraped {len(crawled_data)} pages.\")\n",
    "\n",
    "    # Save data in JSONL format\n",
    "    save_data_to_llm_format(crawled_data, output_file=\"training_data.jsonl\")\n",
    "    print(\"Data saved in LLM training-friendly format: training_data.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30d913-34e8-4be6-bb61-866a8323279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "To implement embeddings locally using the mlx_lm library, you can follow the steps below. This script will use mlx_lm to create embeddings for Java, shell scripts, and SQL stored procedures, store them in a vector database, and allow for semantic search.\n",
    "\n",
    "Prerequisites\n",
    "\n",
    "\t1.\tInstall the required libraries:\n",
    "\n",
    "pip install mlx-lm sentence-transformers faiss-cpu\n",
    "\n",
    "\n",
    "\t2.\tEnsure your local environment has access to the source code files.\n",
    "\n",
    "Script: Embedding and Semantic Search\n",
    "\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the model for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight model for embedding\n",
    "\n",
    "# Define directories and file types\n",
    "source_code_dir = './source_code'  # Directory containing Java, shell, SQL files\n",
    "file_types = ['.java', '.sh', '.sql']  # File extensions to process\n",
    "\n",
    "# Step 1: Extract content from source code files\n",
    "def extract_files(directory, file_extensions):\n",
    "    file_contents = []\n",
    "    file_paths = []\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if any(file.endswith(ext) for ext in file_extensions):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    file_contents.append(content)\n",
    "                    file_paths.append(file_path)\n",
    "    \n",
    "    return file_contents, file_paths\n",
    "\n",
    "# Step 2: Generate embeddings for source code\n",
    "def generate_embeddings(contents, model):\n",
    "    return model.encode(contents, convert_to_numpy=True)\n",
    "\n",
    "# Step 3: Create FAISS index for embeddings\n",
    "def create_faiss_index(embeddings):\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# Step 4: Perform semantic search\n",
    "def search(query, model, index, contents, paths, top_k=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(len(indices[0])):\n",
    "        results.append({\n",
    "            \"file_path\": paths[indices[0][i]],\n",
    "            \"content\": contents[indices[0][i]],\n",
    "            \"distance\": distances[0][i]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Extracting source code...\")\n",
    "    file_contents, file_paths = extract_files(source_code_dir, file_types)\n",
    "    \n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = generate_embeddings(file_contents, model)\n",
    "    \n",
    "    print(\"Creating FAISS index...\")\n",
    "    index = create_faiss_index(embeddings)\n",
    "    \n",
    "    # Query the index\n",
    "    print(\"Ready for semantic search. Enter your query:\")\n",
    "    while True:\n",
    "        query = input(\"Query (or type 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        results = search(query, model, index, file_contents, file_paths)\n",
    "        print(\"\\nTop results:\")\n",
    "        for res in results:\n",
    "            print(f\"File: {res['file_path']}\")\n",
    "            print(f\"Distance: {res['distance']}\")\n",
    "            print(\"Snippet:\")\n",
    "            print(res['content'][:500], \"...\\n\")\n",
    "\n",
    "How It Works:\n",
    "\n",
    "\t1.\tExtract Files:\n",
    "\t•\tScans the specified directory for .java, .sh, and .sql files.\n",
    "\t•\tReads and stores their contents.\n",
    "\t2.\tGenerate Embeddings:\n",
    "\t•\tConverts each file’s content into a vector using SentenceTransformer.\n",
    "\t3.\tCreate FAISS Index:\n",
    "\t•\tEmbeddings are stored in a FAISS index for fast similarity searches.\n",
    "\t4.\tSemantic Search:\n",
    "\t•\tUsers can input queries, and the script retrieves the top k most similar files/content snippets.\n",
    "\n",
    "Usage\n",
    "\n",
    "\t1.\tPlace your source code files in the ./source_code directory.\n",
    "\t2.\tRun the script.\n",
    "\t3.\tEnter a query like:\n",
    "\t•\t\"Find the retry logic implementation\"\n",
    "\t•\t\"How is error handling performed?\"\n",
    "\t4.\tThe script will return the most relevant files and snippets.\n",
    "\n",
    "This approach provides an efficient way to work with and analyze large codebases locally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7036f1-9af0-4eb0-a227-aec62660059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from mlx_lm import LMModel, Trainer, DataProcessor, Tokenizer\n",
    "\n",
    "# Paths\n",
    "SOURCE_CODE_DIR = './legacy_code'\n",
    "MODEL_DIR = './model_output'\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "\n",
    "# File types to process\n",
    "FILE_TYPES = ['.java', '.sh', '.sql']\n",
    "\n",
    "\n",
    "# Step 1: Extract Source Code\n",
    "def extract_files(directory, file_types):\n",
    "    \"\"\"\n",
    "    Extract source code from files in a directory.\n",
    "    \"\"\"\n",
    "    file_contents = []\n",
    "    file_paths = []\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if any(file.endswith(ext) for ext in file_types):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        file_contents.append(content)\n",
    "                        file_paths.append(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "    return file_contents, file_paths\n",
    "\n",
    "\n",
    "# Step 2: Preprocess Code\n",
    "def preprocess_code(code_snippets):\n",
    "    \"\"\"\n",
    "    Clean and tokenize source code snippets.\n",
    "    \"\"\"\n",
    "    processed_snippets = []\n",
    "    for snippet in code_snippets:\n",
    "        # Remove comments and extra whitespace\n",
    "        snippet = re.sub(r'//.*?(\\n|$)|/\\*.*?\\*/', '', snippet, flags=re.DOTALL)\n",
    "        snippet = re.sub(r'\\s+', ' ', snippet).strip()\n",
    "        processed_snippets.append(snippet)\n",
    "    return processed_snippets\n",
    "\n",
    "\n",
    "# Step 3: Train the Model\n",
    "def train_model(train_texts, eval_texts):\n",
    "    \"\"\"\n",
    "    Train a model using mlx_lm.\n",
    "    \"\"\"\n",
    "    # Initialize Tokenizer\n",
    "    tokenizer = Tokenizer(MODEL_NAME)\n",
    "\n",
    "    # Preprocess and tokenize the data\n",
    "    train_data = DataProcessor(train_texts, tokenizer)\n",
    "    eval_data = DataProcessor(eval_texts, tokenizer)\n",
    "\n",
    "    # Initialize Model\n",
    "    model = LMModel(model_name=MODEL_NAME)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_data=train_data,\n",
    "        eval_data=eval_data,\n",
    "        output_dir=MODEL_DIR,\n",
    "        epochs=3,\n",
    "        batch_size=8,\n",
    "        learning_rate=5e-5,\n",
    "        save_steps=100\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    trainer.save_model()\n",
    "\n",
    "\n",
    "# Step 4: Generate Suggestions\n",
    "def generate_suggestions(input_code, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate modernization suggestions for input code.\n",
    "    \"\"\"\n",
    "    processed_code = tokenizer.tokenize([input_code])\n",
    "    predictions = model.predict(processed_code)\n",
    "    return tokenizer.detokenize(predictions)\n",
    "\n",
    "\n",
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Extracting source code...\")\n",
    "    code_snippets, file_paths = extract_files(SOURCE_CODE_DIR, FILE_TYPES)\n",
    "\n",
    "    if not code_snippets:\n",
    "        print(\"No files found for processing. Please check the source directory.\")\n",
    "        exit()\n",
    "\n",
    "    print(\"Preprocessing code...\")\n",
    "    preprocessed_snippets = preprocess_code(code_snippets)\n",
    "\n",
    "    # Split data for training and evaluation\n",
    "    train_size = int(0.8 * len(preprocessed_snippets))\n",
    "    train_texts = preprocessed_snippets[:train_size]\n",
    "    eval_texts = preprocessed_snippets[train_size:]\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    train_model(train_texts, eval_texts)\n",
    "\n",
    "    print(\"Loading model and tokenizer for inference...\")\n",
    "    tokenizer = Tokenizer(MODEL_NAME)\n",
    "    model = LMModel.load(MODEL_DIR)\n",
    "\n",
    "    print(\"Generating suggestions...\")\n",
    "    test_snippet = \"public void legacyMethod() { int x = 10; System.out.println(x); }\"\n",
    "    suggestions = generate_suggestions(test_snippet, model, tokenizer)\n",
    "    print(f\"Suggestions for modernization:\\n{suggestions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6848b-6f65-4b07-97c1-1166baadb152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Initialize Safari WebDriver\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "# Base URL to scrape\n",
    "BASE_URL = \"https://example.com\"  # Replace with the URL you want to scrape\n",
    "OUTPUT_FILE = \"llm_training_dataset.json\"\n",
    "\n",
    "# Track visited URLs to avoid duplication\n",
    "visited_urls = set()\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = []\n",
    "\n",
    "def extract_content(driver):\n",
    "    \"\"\"\n",
    "    Extracts all text content from the current webpage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return driver.find_element(By.TAG_NAME, \"body\").text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def save_dataset(dataset, output_file):\n",
    "    \"\"\"\n",
    "    Saves the dataset to a JSON file.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(dataset, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def visit_and_extract(url):\n",
    "    \"\"\"\n",
    "    Visits a URL, extracts content, and explores its links.\n",
    "    \"\"\"\n",
    "    if url in visited_urls:\n",
    "        return\n",
    "    print(f\"Visiting: {url}\")\n",
    "    visited_urls.add(url)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Allow time for the page to load\n",
    "\n",
    "        # Extract content\n",
    "        content = extract_content(driver)\n",
    "        if content.strip():\n",
    "            dataset.append({\"url\": url, \"content\": content})\n",
    "\n",
    "        # Find all links on the page\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and href.startswith(BASE_URL):  # Ensure links are within the same domain\n",
    "                visit_and_extract(href)\n",
    "    except Exception as e:\n",
    "        print(f\"Error visiting {url}: {e}\")\n",
    "\n",
    "# Start scraping from the base URL\n",
    "try:\n",
    "    visit_and_extract(BASE_URL)\n",
    "finally:\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    # Save the dataset locally\n",
    "    save_dataset(dataset, OUTPUT_FILE)\n",
    "    print(f\"Dataset saved to {OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
