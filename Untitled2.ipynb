{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cfc1a-4bef-4cce-a959-13bc3195ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "\n",
    "# Initialize SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Comprehensive list of Wells Fargo banking products and services\n",
    "BANKING_PRODUCTS_SERVICES = [\n",
    "    # Credit and Debit Cards\n",
    "    \"credit card\", \"debit card\", \"secured credit card\", \"business credit card\", \"student credit card\", \n",
    "    \"prepaid card\", \"cash back credit card\", \"reward credit card\", \"travel credit card\",\n",
    "    \n",
    "    # Accounts\n",
    "    \"checking account\", \"savings account\", \"business checking account\", \"business savings account\", \n",
    "    \"retirement account\", \"IRA account\", \"CD account\", \"certificate of deposit\", \n",
    "    \"trust account\", \"custodial account\", \"high-yield savings account\",\n",
    "    \n",
    "    # Loans and Mortgages\n",
    "    \"personal loan\", \"auto loan\", \"home loan\", \"mortgage\", \"mortgage refinancing\", \n",
    "    \"home equity loan\", \"HELOC\", \"student loan\", \"small business loan\", \"paycheck protection program\",\n",
    "    \"commercial loan\",\n",
    "    \n",
    "    # Online and Mobile Banking\n",
    "    \"online banking\", \"mobile app\", \"mobile banking\", \"bill pay\", \"account alerts\", \n",
    "    \"mobile check deposit\", \"direct deposit\", \"e-statements\", \"fund transfer\", \"mobile wallet\",\n",
    "    \"digital payments\", \"password reset\", \"account recovery\",\n",
    "    \n",
    "    # Investment Services\n",
    "    \"investment account\", \"brokerage account\", \"mutual funds\", \"ETFs\", \"stocks\", \"bonds\", \n",
    "    \"retirement planning\", \"college savings plan\", \"annuity\", \"wealth management\",\n",
    "    \"financial planning\",\n",
    "    \n",
    "    # Payments\n",
    "    \"Zelle\", \"ACH transfer\", \"wire transfer\", \"payment gateway\", \"merchant services\",\n",
    "    \n",
    "    # Customer Support and Fraud\n",
    "    \"customer service\", \"chat support\", \"call center\", \"branch service\", \n",
    "    \"fraud detection\", \"account fraud recovery\", \"identity theft protection\",\n",
    "    \n",
    "    # ATM and Branch Services\n",
    "    \"ATM withdrawal\", \"ATM deposit\", \"branch banking\", \"safe deposit box\", \"notary services\",\n",
    "    \n",
    "    # Insurance\n",
    "    \"life insurance\", \"auto insurance\", \"home insurance\", \"renter's insurance\", \n",
    "    \"disability insurance\", \"travel insurance\",\n",
    "    \n",
    "    # Business Banking Services\n",
    "    \"business banking\", \"merchant account\", \"treasury management\", \"payroll services\",\n",
    "    \"business line of credit\", \"equipment financing\",\n",
    "]\n",
    "\n",
    "# Function to fetch comments based on the given node structure\n",
    "def fetch_comments(url):\n",
    "    # Path to GeckoDriver (replace with your path)\n",
    "    driver_path = \"./geckodriver\"\n",
    "    service = Service(driver_path, log_path=os.devnull)  # Suppress GeckoDriver logs\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run browser in headless mode\n",
    "\n",
    "    # Initialize Firefox WebDriver\n",
    "    driver = webdriver.Firefox(service=service, options=options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(5)  # Wait for the page to load completely\n",
    "        disqus_thread = driver.find_element(By.ID, \"disqus_thread\")\n",
    "        comments = []\n",
    "        blocks = disqus_thread.find_elements(By.CLASS_NAME, \"block\")\n",
    "        for block in blocks:\n",
    "            p_tag = block.find_element(By.TAG_NAME, \"p\")  # Get the <p> tag within the block\n",
    "            comments.append(p_tag.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {e}\")\n",
    "        comments = []\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return comments\n",
    "\n",
    "# Function to classify comments by banking products and services\n",
    "def classify_comment(comment, products_services):\n",
    "    classifications = []\n",
    "    for product in products_services:\n",
    "        if product.lower() in comment.lower():\n",
    "            classifications.append(product)\n",
    "    return classifications\n",
    "\n",
    "# Function to analyze sentiment using TextBlob\n",
    "def analyze_sentiment(comment):\n",
    "    polarity = TextBlob(comment).sentiment.polarity\n",
    "    if polarity > 0.2:\n",
    "        return \"positive\", polarity\n",
    "    elif polarity < -0.2:\n",
    "        return \"negative\", polarity\n",
    "    else:\n",
    "        return \"neutral\", polarity\n",
    "\n",
    "# Function to analyze comments with SpaCy and TextBlob\n",
    "def analyze_comments(comments):\n",
    "    summary = []\n",
    "    for comment in comments:\n",
    "        # Analyze sentiment\n",
    "        sentiment, score = analyze_sentiment(comment)\n",
    "\n",
    "        # Extract entities using SpaCy\n",
    "        doc = nlp(comment)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "        # Classify the comment by banking products and services\n",
    "        classifications = classify_comment(comment, BANKING_PRODUCTS_SERVICES)\n",
    "\n",
    "        # Add the analysis results to the summary\n",
    "        summary.append({\n",
    "            \"comment\": comment\n",
    "            \n",
    "        })\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Main script to fetch comments, analyze, and save as JSON\n",
    "def main():\n",
    "    # Downdetector page URL\n",
    "    url = 'https://downdetector.com/status/wells-fargo/'  # Replace with actual URL\n",
    "\n",
    "    # Fetch comments\n",
    "    print(\"Fetching comments...\")\n",
    "    comments = fetch_comments(url)\n",
    "    if not comments:\n",
    "        print(\"No comments fetched. Check your connection or site structure.\")\n",
    "        return\n",
    "\n",
    "    # Analyze comments\n",
    "    print(\"Analyzing comments...\")\n",
    "    analysis_results = analyze_comments(comments)\n",
    "\n",
    "    # Save the analysis results as JSON\n",
    "    output_file = \"wellsfargo_analysis.json\"\n",
    "    with open(output_file, \"w\") as json_file:\n",
    "        json.dump(analysis_results, json_file, indent=4)\n",
    "\n",
    "    print(f\"Analysis completed. Results saved to '{output_file}'.\")\n",
    "    print(json.dumps(analysis_results, indent=4))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0999b32-48a1-4ef4-a8bc-796843189b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import spacy\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "class WellsFargoCommentAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Load SpaCy model with additional components\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\")  # Using larger model for better analysis\n",
    "        \n",
    "        # Add custom sentiment lexicon\n",
    "        self.sentiment_lexicon = self._create_sentiment_lexicon()\n",
    "\n",
    "        # Comprehensive list of Wells Fargo banking products and services\n",
    "        self.BANKING_PRODUCTS_SERVICES = [\n",
    "            # Credit and Debit Cards\n",
    "            \"credit card\", \"debit card\", \"secured credit card\", \"business credit card\", \n",
    "            \"student credit card\", \"prepaid card\", \"cash back credit card\", \n",
    "            \"reward credit card\", \"travel credit card\",\n",
    "            \n",
    "            # Accounts\n",
    "            \"checking account\", \"savings account\", \"business checking\", \n",
    "            \"retirement account\", \"IRA\", \"CD\", \"certificate of deposit\",\n",
    "            \n",
    "            # Loans and Mortgages\n",
    "            \"personal loan\", \"auto loan\", \"home loan\", \"mortgage\", \n",
    "            \"mortgage refinancing\", \"home equity loan\", \"HELOC\", \n",
    "            \"student loan\", \"small business loan\",\n",
    "            \n",
    "            # Online and Mobile Banking\n",
    "            \"online banking\", \"mobile app\", \"mobile banking\", \"bill pay\", \n",
    "            \"mobile check deposit\", \"direct deposit\", \"e-statements\", \n",
    "            \"fund transfer\", \"mobile wallet\",\n",
    "            \n",
    "            # Payment Services\n",
    "            \"Zelle\", \"wire transfer\", \"ACH transfer\",\n",
    "            \n",
    "            # Specific Wells Fargo Terms\n",
    "            \"wells fargo\", \"account\", \"branch\", \"customer service\"\n",
    "        ]\n",
    "\n",
    "    def _create_sentiment_lexicon(self):\n",
    "        \"\"\"\n",
    "        Create a custom sentiment lexicon using SpaCy's linguistic features\n",
    "        \"\"\"\n",
    "        # More comprehensive sentiment lexicon\n",
    "        return {\n",
    "            # Negative words\n",
    "            'negative': {\n",
    "                'bad', 'worst', 'terrible', 'horrible', 'awful', 'poor', 'frustrating', \n",
    "                'slow', 'incompetent', 'useless', 'problematic', 'fail', 'failed', \n",
    "                'failure', 'issue', 'problem', 'error', 'broken', 'disappointed', \n",
    "                'disastrous', 'unhelpful', 'complicated', 'confusing'\n",
    "            },\n",
    "            # Positive words\n",
    "            'positive': {\n",
    "                'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', \n",
    "                'helpful', 'easy', 'smooth', 'quick', 'efficient', 'best', 'perfect', \n",
    "                'awesome', 'outstanding', 'impressive', 'convenient', 'reliable'\n",
    "            },\n",
    "            # Intensifiers\n",
    "            'intensifiers': {\n",
    "                'very', 'extremely', 'incredibly', 'absolutely', 'totally', 'completely'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def analyze_sentiment(self, doc):\n",
    "        \"\"\"\n",
    "        Custom sentiment analysis using SpaCy's linguistic analysis\n",
    "        \"\"\"\n",
    "        # Initialize sentiment score\n",
    "        sentiment_score = 0\n",
    "        \n",
    "        # Track intensifiers and negations\n",
    "        intensifier_multiplier = 1\n",
    "        negation_multiplier = 1\n",
    "        \n",
    "        for token in doc:\n",
    "            # Check for intensifiers\n",
    "            if token.text.lower() in self.sentiment_lexicon['intensifiers']:\n",
    "                intensifier_multiplier = 2\n",
    "            \n",
    "            # Check for negations\n",
    "            if token.dep_ == 'neg':\n",
    "                negation_multiplier = -1\n",
    "            \n",
    "            # Check for sentiment words\n",
    "            if token.text.lower() in self.sentiment_lexicon['negative']:\n",
    "                sentiment_score -= (1 * intensifier_multiplier * negation_multiplier)\n",
    "            \n",
    "            if token.text.lower() in self.sentiment_lexicon['positive']:\n",
    "                sentiment_score += (1 * intensifier_multiplier * negation_multiplier)\n",
    "        \n",
    "        # Normalize sentiment score\n",
    "        if sentiment_score > 1:\n",
    "            sentiment_label = 'positive'\n",
    "        elif sentiment_score < -1:\n",
    "            sentiment_label = 'negative'\n",
    "        else:\n",
    "            sentiment_label = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'label': sentiment_label,\n",
    "            'score': sentiment_score\n",
    "        }\n",
    "\n",
    "    def fetch_comments(self, url):\n",
    "        \"\"\"Fetch comments from a given URL using Selenium\"\"\"\n",
    "        driver_path = \"./geckodriver\"\n",
    "        service = Service(driver_path, log_path=os.devnull)\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")\n",
    "\n",
    "        driver = webdriver.Firefox(service=service, options=options)\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            driver.implicitly_wait(5)\n",
    "            \n",
    "            comments = []\n",
    "            disqus_thread = driver.find_element(By.ID, \"disqus_thread\")\n",
    "            comment_blocks = disqus_thread.find_elements(By.CLASS_NAME, \"block\")\n",
    "            \n",
    "            for block in comment_blocks:\n",
    "                p_tag = block.find_element(By.TAG_NAME, \"p\")\n",
    "                comments.append(p_tag.text)\n",
    "            \n",
    "            return comments\n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping: {e}\")\n",
    "            return []\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    def analyze_comment(self, comment):\n",
    "        \"\"\"Comprehensive analysis of a single comment\"\"\"\n",
    "        # Process comment with SpaCy\n",
    "        doc = self.nlp(comment)\n",
    "        \n",
    "        # Sentiment Analysis\n",
    "        sentiment = self.analyze_sentiment(doc)\n",
    "        \n",
    "        # Detect products/services\n",
    "        detected_products = self._detect_products(comment)\n",
    "        \n",
    "        # Named Entity Recognition\n",
    "        entities = [{\"text\": ent.text, \"type\": ent.label_} for ent in doc.ents]\n",
    "        \n",
    "        # Dependency Parsing Insights\n",
    "        dependencies = [\n",
    "            {\n",
    "                'token': token.text, \n",
    "                'dependency': token.dep_, \n",
    "                'head': token.head.text\n",
    "            } for token in doc\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"original_comment\": comment,\n",
    "            \"sentiment\": sentiment,\n",
    "            \"detected_products\": detected_products,\n",
    "            \"named_entities\": entities,\n",
    "            \"dependencies\": dependencies\n",
    "        }\n",
    "\n",
    "    def _detect_products(self, comment):\n",
    "        \"\"\"Efficiently detect products mentioned in a comment\"\"\"\n",
    "        comment_lower = comment.lower()\n",
    "        return [\n",
    "            product for product in self.BANKING_PRODUCTS_SERVICES \n",
    "            if product.lower() in comment_lower\n",
    "        ]\n",
    "\n",
    "    def analyze_comments(self, comments):\n",
    "        \"\"\"Analyze multiple comments and generate comprehensive summary\"\"\"\n",
    "        analysis_results = []\n",
    "        product_sentiment_summary = defaultdict(lambda: {\n",
    "            \"positive\": 0, \n",
    "            \"neutral\": 0, \n",
    "            \"negative\": 0,\n",
    "            \"total_mentions\": 0\n",
    "        })\n",
    "        \n",
    "        overall_sentiment_summary = {\n",
    "            \"positive\": 0,\n",
    "            \"neutral\": 0,\n",
    "            \"negative\": 0\n",
    "        }\n",
    "        \n",
    "        for comment in comments:\n",
    "            analysis = self.analyze_comment(comment)\n",
    "            analysis_results.append(analysis)\n",
    "            \n",
    "            # Update overall sentiment summary\n",
    "            sentiment_label = analysis['sentiment']['label']\n",
    "            overall_sentiment_summary[sentiment_label] += 1\n",
    "            \n",
    "            # Update product sentiment summary\n",
    "            for product in analysis['detected_products']:\n",
    "                product_summary = product_sentiment_summary[product]\n",
    "                product_summary[sentiment_label] += 1\n",
    "                product_summary['total_mentions'] += 1\n",
    "        \n",
    "        return {\n",
    "            \"total_comments\": len(comments),\n",
    "            \"individual_analyses\": analysis_results,\n",
    "            \"overall_sentiment_summary\": overall_sentiment_summary,\n",
    "            \"product_sentiment_summary\": dict(product_sentiment_summary)\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Initialize the analyzer\n",
    "    analyzer = WellsFargoCommentAnalyzer()\n",
    "    \n",
    "    # Downdetector page URL\n",
    "    url = 'https://downdetector.com/status/wells-fargo/'  # Replace with actual URL\n",
    "\n",
    "    # Fetch comments\n",
    "    print(\"Fetching comments...\")\n",
    "    comments = analyzer.fetch_comments(url)\n",
    "    if not comments:\n",
    "        print(\"No comments fetched. Check your connection or site structure.\")\n",
    "        return\n",
    "\n",
    "    # Analyze comments\n",
    "    print(\"Analyzing comments...\")\n",
    "    analysis_results = analyzer.analyze_comments(comments)\n",
    "\n",
    "    # Save the analysis results as JSON\n",
    "    output_file = \"wellsfargo_analysis.json\"\n",
    "    with open(output_file, \"w\") as json_file:\n",
    "        json.dump(analysis_results, json_file, indent=4)\n",
    "\n",
    "    print(f\"Analysis completed. Results saved to '{output_file}'.\")\n",
    "    print(json.dumps(analysis_results, indent=4))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6591ca26-4a02-42ca-b252-177e4eee0399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import spacy\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "class WellsFargoCommentAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Load SpaCy model with additional components\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")  # Using larger model for better analysis\n",
    "        \n",
    "        # Add custom sentiment lexicon\n",
    "        self.sentiment_lexicon = self._create_sentiment_lexicon()\n",
    "\n",
    "        # Comprehensive list of Wells Fargo banking products and services\n",
    "        self.BANKING_PRODUCTS_SERVICES = [\n",
    "            # Credit and Debit Cards\n",
    "            \"credit card\", \"debit card\", \"secured credit card\", \"business credit card\", \n",
    "            \"student credit card\", \"prepaid card\", \"cash back credit card\", \n",
    "            \"reward credit card\", \"travel credit card\",\n",
    "            \n",
    "            # Accounts\n",
    "            \"checking account\", \"savings account\", \"business checking\", \n",
    "            \"retirement account\", \"IRA\", \"CD\", \"certificate of deposit\",\n",
    "            \n",
    "            # Loans and Mortgages\n",
    "            \"personal loan\", \"auto loan\", \"home loan\", \"mortgage\", \n",
    "            \"mortgage refinancing\", \"home equity loan\", \"HELOC\", \n",
    "            \"student loan\", \"small business loan\",\n",
    "            \n",
    "            # Online and Mobile Banking\n",
    "            \"online banking\", \"mobile app\", \"mobile banking\", \"bill pay\", \n",
    "            \"mobile check deposit\", \"direct deposit\", \"e-statements\", \n",
    "            \"fund transfer\", \"mobile wallet\",\n",
    "            \n",
    "            # Payment Services\n",
    "            \"Zelle\", \"wire transfer\", \"ACH transfer\",\n",
    "            \n",
    "            # Specific Wells Fargo Terms\n",
    "            \"wells fargo\", \"account\", \"branch\", \"customer service\"\n",
    "        ]\n",
    "\n",
    "    def _create_sentiment_lexicon(self):\n",
    "        \"\"\"\n",
    "        Create a custom sentiment lexicon using SpaCy's linguistic features\n",
    "        \"\"\"\n",
    "        # More comprehensive sentiment lexicon\n",
    "        return {\n",
    "            # Negative words\n",
    "            'negative': {\n",
    "                'bad', 'worst', 'terrible', 'horrible', 'awful', 'poor', 'frustrating', \n",
    "                'slow', 'incompetent', 'useless', 'problematic', 'fail', 'failed', \n",
    "                'failure', 'issue', 'problem', 'error', 'broken', 'disappointed', \n",
    "                'disastrous', 'unhelpful', 'complicated', 'confusing'\n",
    "            },\n",
    "            # Positive words\n",
    "            'positive': {\n",
    "                'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', \n",
    "                'helpful', 'easy', 'smooth', 'quick', 'efficient', 'best', 'perfect', \n",
    "                'awesome', 'outstanding', 'impressive', 'convenient', 'reliable'\n",
    "            },\n",
    "            # Intensifiers\n",
    "            'intensifiers': {\n",
    "                'very', 'extremely', 'incredibly', 'absolutely', 'totally', 'completely'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def analyze_sentiment(self, doc):\n",
    "        \"\"\"\n",
    "        Custom sentiment analysis using SpaCy's linguistic analysis\n",
    "        \"\"\"\n",
    "        # Initialize sentiment score\n",
    "        sentiment_score = 0\n",
    "        \n",
    "        # Track intensifiers and negations\n",
    "        intensifier_multiplier = 1\n",
    "        negation_multiplier = 1\n",
    "        \n",
    "        for token in doc:\n",
    "            # Check for intensifiers\n",
    "            if token.text.lower() in self.sentiment_lexicon['intensifiers']:\n",
    "                intensifier_multiplier = 2\n",
    "            \n",
    "            # Check for negations\n",
    "            if token.dep_ == 'neg':\n",
    "                negation_multiplier = -1\n",
    "            \n",
    "            # Check for sentiment words\n",
    "            if token.text.lower() in self.sentiment_lexicon['negative']:\n",
    "                sentiment_score -= (1 * intensifier_multiplier * negation_multiplier)\n",
    "            \n",
    "            if token.text.lower() in self.sentiment_lexicon['positive']:\n",
    "                sentiment_score += (1 * intensifier_multiplier * negation_multiplier)\n",
    "        \n",
    "        # Normalize sentiment score\n",
    "        if sentiment_score > 1:\n",
    "            sentiment_label = 'positive'\n",
    "        elif sentiment_score < -1:\n",
    "            sentiment_label = 'negative'\n",
    "        else:\n",
    "            sentiment_label = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'label': sentiment_label,\n",
    "            'score': sentiment_score\n",
    "        }\n",
    "\n",
    "    def fetch_comments(self, url):\n",
    "        \"\"\"Fetch comments from a given URL using Selenium\"\"\"\n",
    "        driver_path = \"./geckodriver\"\n",
    "        service = Service(driver_path, log_path=os.devnull)\n",
    "        options = Options()\n",
    "        #options.add_argument(\"--headless\")\n",
    "\n",
    "        driver = webdriver.Firefox(service=service, options=options)\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            driver.implicitly_wait(5)\n",
    "            \n",
    "            comments = []\n",
    "            disqus_thread = driver.find_element(By.ID, \"disqus_thread\")\n",
    "            comment_blocks = disqus_thread.find_elements(By.CLASS_NAME, \"block\")\n",
    "            \n",
    "            for block in comment_blocks:\n",
    "                p_tag = block.find_element(By.TAG_NAME, \"p\")\n",
    "                comments.append(p_tag.text)\n",
    "            \n",
    "            return comments\n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping: {e}\")\n",
    "            return []\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    def analyze_comment(self, comment):\n",
    "        \"\"\"Comprehensive analysis of a single comment\"\"\"\n",
    "        # Process comment with SpaCy\n",
    "        doc = self.nlp(comment)\n",
    "        \n",
    "        # Sentiment Analysis\n",
    "        sentiment = self.analyze_sentiment(doc)\n",
    "        \n",
    "        # Detect products/services\n",
    "        detected_products = self._detect_products(comment)\n",
    "        \n",
    "        # Named Entity Recognition\n",
    "        entities = [{\"text\": ent.text, \"type\": ent.label_} for ent in doc.ents]\n",
    "        \n",
    "        # Dependency Parsing Insights\n",
    "        dependencies = [\n",
    "            {\n",
    "                'token': token.text, \n",
    "                'dependency': token.dep_, \n",
    "                'head': token.head.text\n",
    "            } for token in doc\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"original_comment\": comment,\n",
    "            \"sentiment\": sentiment,\n",
    "            \"detected_products\": detected_products,\n",
    "            \"named_entities\": entities,\n",
    "            \"dependencies\": dependencies\n",
    "        }\n",
    "\n",
    "    def _detect_products(self, comment):\n",
    "        \"\"\"Efficiently detect products mentioned in a comment\"\"\"\n",
    "        comment_lower = comment.lower()\n",
    "        return [\n",
    "            product for product in self.BANKING_PRODUCTS_SERVICES \n",
    "            if product.lower() in comment_lower\n",
    "        ]\n",
    "\n",
    "    def analyze_comments(self, comments):\n",
    "        \"\"\"Analyze multiple comments and generate comprehensive summary\"\"\"\n",
    "        analysis_results = []\n",
    "        product_sentiment_summary = defaultdict(lambda: {\n",
    "            \"positive\": 0, \n",
    "            \"neutral\": 0, \n",
    "            \"negative\": 0,\n",
    "            \"total_mentions\": 0\n",
    "        })\n",
    "        \n",
    "        overall_sentiment_summary = {\n",
    "            \"positive\": 0,\n",
    "            \"neutral\": 0,\n",
    "            \"negative\": 0\n",
    "        }\n",
    "        \n",
    "        for comment in comments:\n",
    "            analysis = self.analyze_comment(comment)\n",
    "            analysis_results.append(analysis)\n",
    "            \n",
    "            # Update overall sentiment summary\n",
    "            sentiment_label = analysis['sentiment']['label']\n",
    "            overall_sentiment_summary[sentiment_label] += 1\n",
    "            \n",
    "            # Update product sentiment summary\n",
    "            for product in analysis['detected_products']:\n",
    "                product_summary = product_sentiment_summary[product]\n",
    "                product_summary[sentiment_label] += 1\n",
    "                product_summary['total_mentions'] += 1\n",
    "        \n",
    "        return {\n",
    "            \"total_comments\": len(comments),\n",
    "            \"individual_analyses\": analysis_results,\n",
    "            \"overall_sentiment_summary\": overall_sentiment_summary,\n",
    "            \"product_sentiment_summary\": dict(product_sentiment_summary)\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Initialize the analyzer\n",
    "    analyzer = WellsFargoCommentAnalyzer()\n",
    "    \n",
    "    # Downdetector page URL\n",
    "    url = 'https://downdetector.com/status/wells-fargo/'  # Replace with actual URL\n",
    "\n",
    "    # Fetch comments\n",
    "    print(\"Fetching comments...\")\n",
    "    comments = analyzer.fetch_comments(url)\n",
    "    if not comments:\n",
    "        print(\"No comments fetched. Check your connection or site structure.\")\n",
    "        return\n",
    "\n",
    "    # Analyze comments\n",
    "    print(\"Analyzing comments...\")\n",
    "    analysis_results = analyzer.analyze_comments(comments)\n",
    "\n",
    "    # Save the analysis results as JSON\n",
    "    output_file = \"wellsfargo_analysis.json\"\n",
    "    with open(output_file, \"w\") as json_file:\n",
    "        json.dump(analysis_results, json_file, indent=4)\n",
    "\n",
    "    print(f\"Analysis completed. Results saved to '{output_file}'.\")\n",
    "    print(json.dumps(analysis_results, indent=4))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db4f889a-7bfb-4301-a94a-f83fe11f8185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Initialize Hugging Face Sentiment Analysis Pipeline\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m sentiment_analyzer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentiment-analysis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Comprehensive list of Wells Fargo banking products and services\u001b[39;00m\n\u001b[1;32m     17\u001b[0m BANKING_PRODUCTS_SERVICES \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcredit card\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebit card\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monline banking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmobile app\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchecking account\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msavings account\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersonal loan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto loan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmortgage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwire transfer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZelle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer service\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mATM withdrawal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvestment account\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/hackathpn 2024/presentations/lib/python3.11/site-packages/transformers/pipelines/__init__.py:926\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    925\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 926\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    937\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/Documents/hackathpn 2024/presentations/lib/python3.11/site-packages/transformers/pipelines/base.py:240\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    246\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[0;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize Hugging Face Sentiment Analysis Pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Comprehensive list of Wells Fargo banking products and services\n",
    "BANKING_PRODUCTS_SERVICES = [\n",
    "    \"credit card\", \"debit card\", \"online banking\", \"mobile app\", \"checking account\",\n",
    "    \"savings account\", \"personal loan\", \"auto loan\", \"mortgage\", \"wire transfer\",\n",
    "    \"Zelle\", \"customer service\", \"ATM withdrawal\", \"investment account\"\n",
    "]\n",
    "\n",
    "# Function to fetch comments based on the given node structure\n",
    "def fetch_comments(url):\n",
    "    # Path to GeckoDriver (replace with your path)\n",
    "    driver_path = \"./geckodriver\"\n",
    "    service = Service(driver_path, log_path=os.devnull)  # Suppress GeckoDriver logs\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run browser in headless mode\n",
    "\n",
    "    # Initialize Firefox WebDriver\n",
    "    driver = webdriver.Firefox(service=service, options=options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(5)  # Wait for the page to load completely\n",
    "        disqus_thread = driver.find_element(By.ID, \"disqus_thread\")\n",
    "        comments = []\n",
    "        blocks = disqus_thread.find_elements(By.CLASS_NAME, \"block\")\n",
    "        for block in blocks:\n",
    "            p_tag = block.find_element(By.TAG_NAME, \"p\")  # Get the <p> tag within the block\n",
    "            comments.append(p_tag.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {e}\")\n",
    "        comments = []\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return comments\n",
    "\n",
    "# Function to classify comments by banking products and services\n",
    "def classify_comment(comment, products_services):\n",
    "    classifications = []\n",
    "    for product in products_services:\n",
    "        if product.lower() in comment.lower():\n",
    "            classifications.append(product)\n",
    "    return classifications\n",
    "\n",
    "# Function to analyze sentiment using Hugging Face Transformers\n",
    "def analyze_sentiment(comment):\n",
    "    result = sentiment_analyzer(comment)[0]\n",
    "    sentiment = result[\"label\"].lower()  # \"POSITIVE\" or \"NEGATIVE\"\n",
    "    score = result[\"score\"]  # Confidence score\n",
    "    return sentiment, score\n",
    "\n",
    "# Function to analyze comments with SpaCy and Transformers\n",
    "def analyze_comments(comments):\n",
    "    summary = []\n",
    "    for comment in comments:\n",
    "        # Analyze sentiment\n",
    "        sentiment, score = analyze_sentiment(comment)\n",
    "\n",
    "        # Extract entities using SpaCy\n",
    "        doc = nlp(comment)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "        # Classify the comment by banking products and services\n",
    "        classifications = classify_comment(comment, BANKING_PRODUCTS_SERVICES)\n",
    "\n",
    "        # Add the analysis results to the summary\n",
    "        summary.append({\n",
    "            \"comment\": comment,\n",
    "            \"sentiment_score\": score,\n",
    "            \"sentiment\": sentiment,\n",
    "            \"entities\": entities,\n",
    "            \"classifications\": classifications\n",
    "        })\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Main script to fetch comments, analyze, and save as JSON\n",
    "def main():\n",
    "    # Downdetector page URL\n",
    "    url = 'https://downdetector.com/status/wells-fargo/'  # Replace with actual URL\n",
    "\n",
    "    # Fetch comments\n",
    "    print(\"Fetching comments...\")\n",
    "    comments = fetch_comments(url)\n",
    "    if not comments:\n",
    "        print(\"No comments fetched. Check your connection or site structure.\")\n",
    "        return\n",
    "\n",
    "    # Analyze comments\n",
    "    print(\"Analyzing comments...\")\n",
    "    analysis_results = analyze_comments(comments)\n",
    "\n",
    "    # Save the analysis results as JSON\n",
    "    output_file = \"wellsfargo_analysis_transformers.json\"\n",
    "    with open(output_file, \"w\") as json_file:\n",
    "        json.dump(analysis_results, json_file, indent=4)\n",
    "\n",
    "    print(f\"Analysis completed. Results saved to '{output_file}'.\")\n",
    "    print(json.dumps(analysis_results, indent=4))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59c3fe27-b6d5-45dd-961d-82e4391f0329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/roshin/nltk_data'\n    - '/Users/roshin/Documents/hackathpn 2024/presentations/nltk_data'\n    - '/Users/roshin/Documents/hackathpn 2024/presentations/share/nltk_data'\n    - '/Users/roshin/Documents/hackathpn 2024/presentations/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/hackathpn 2024/presentations/lib/python3.11/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/hackathpn 2024/presentations/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/roshin/nltk_data'\n    - '/Users/roshin/Documents/hackathpn 2024/presentations/nltk_data'\n    - '/Users/roshin/Documents/hackathpn 2024/presentations/share/nltk_data'\n    - '/Users/roshin/Documents/hackathpn 2024/presentations/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m negative_words \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbad\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msad\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterrible\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhorrible\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mawful\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mangry\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisappointing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Stop words to ignore\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_sentiment\u001b[39m(text):\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    Analyzes sentiment of the input text.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m        str: Sentiment of the text ('Positive', 'Negative', 'Neutral').\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/hackathpn 2024/presentations/lib/python3.11/site-packages/nltk/corpus/util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/Documents/hackathpn 2024/presentations/lib/python3.11/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/Documents/hackathpn 2024/presentations/lib/python3.11/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/hackathpn 2024/presentations/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/roshin/nltk_data'\n    - '/Users/roshin/Documents/hackathpn 2024/presentations/nltk_data'\n    - '/Users/roshin/Documents/hackathpn 2024/presentations/share/nltk_data'\n    - '/Users/roshin/Documents/hackathpn 2024/presentations/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure necessary nltk resources are available\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Predefined lexicon for sentiment analysis\n",
    "positive_words = {\n",
    "    'good', 'happy', 'joy', 'excellent', 'great', 'amazing', 'fantastic', 'love', 'wonderful', 'positive', 'cheerful'\n",
    "}\n",
    "negative_words = {\n",
    "    'bad', 'sad', 'terrible', 'horrible', 'awful', 'hate', 'angry', 'negative', 'poor', 'disappointing', 'upset'\n",
    "}\n",
    "\n",
    "# Stop words to ignore\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Analyzes sentiment of the input text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        str: Sentiment of the text ('Positive', 'Negative', 'Neutral').\n",
    "    \"\"\"\n",
    "    # Tokenize text and convert to lowercase\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Filter out stop words and non-alphanumeric tokens\n",
    "    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Count occurrences of positive and negative words\n",
    "    positive_count = sum(word in positive_words for word in filtered_words)\n",
    "    negative_count = sum(word in negative_words for word in filtered_words)\n",
    "    \n",
    "    # Determine sentiment\n",
    "    if positive_count > negative_count:\n",
    "        return 'Positive'\n",
    "    elif negative_count > positive_count:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    text = input(\"Enter a sentence: \")\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    print(f\"The sentiment of the text is: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd7e7a2e-5e79-4515-81e0-627c402e33e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching comments...\n",
      "Comments extracted successfully:\n",
      "['Abelardo Gonzales I am in San Antonio and I still can not login.', 'Makinu Fox Cat is anyone having issues paying with their card online?', 'Rod munch live in florida where hurricaine is hitting', 'Rod munch Online Banking is temporarily unavailable', 'Rod munch yea Online Banking is temporarily unavailable', 'Kim Marasti can&#x27;t log in to app, says unavailable...anyone else?', 'Glizzy Anyone get their direct deposit yet?', 'Glizzy Nope', 'Daniel Any get early pay yet?', 'Ihop We have a problem. Please try again a little later. when using zelle what is wrong with the browser?????', 'ANonymous Ok, thanks. Just wanted to see if I was the only one with this issue.', 'Jeff R Yes', 'Jeff R Still no deposit, Still waiting. Of course Wells Fargo states Early Direct Deposit is not gauranteed...', 'ANonymous Anybody else missing their early deposit? It&#x27;s an hour after the normal deposit time and I haven&#x27;t seen anything yet.', 'eric me either says unavailable I guess I will wait', 'Tryse Cant use app to mobile deposit', 'Verria-Nichole Can&#x27;t make credit card payments. Not even the transactions will load.']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Function to fetch comments from the specified URL\n",
    "def fetch_comments(url):\n",
    "    # Path to GeckoDriver (replace with your actual path)\n",
    "    driver_path = \"./geckodriver\"\n",
    "    service = Service(driver_path, log_path=os.devnull)  # Suppress GeckoDriver logs\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run browser in headless mode\n",
    "\n",
    "    # Initialize Firefox WebDriver\n",
    "    driver = webdriver.Firefox(service=service, options=options)\n",
    "    comments = []\n",
    "    try:\n",
    "        # Open the URL\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(5)  # Wait for the page to load completely\n",
    "        \n",
    "        # Locate the comments section by ID or class\n",
    "        disqus_thread = driver.find_element(By.ID, \"disqus_thread\")  # Adjust if ID is different\n",
    "        blocks = disqus_thread.find_elements(By.CLASS_NAME, \"block\")  # Adjust if class name is different\n",
    "        \n",
    "        # Extract comments from the located elements\n",
    "        for block in blocks:\n",
    "            p_tag = block.find_element(By.TAG_NAME, \"p\")  # Get the <p> tag within each block\n",
    "            comments.append(p_tag.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Main function to call fetch_comments and display the results\n",
    "def main():\n",
    "    # Specify the URL to scrape (replace with your target URL)\n",
    "    url = 'https://downdetector.com/status/wells-fargo/'  # Replace with actual URL\n",
    "    \n",
    "    print(\"Fetching comments...\")\n",
    "    comments = fetch_comments(url)\n",
    "    if comments:\n",
    "        print(\"Comments extracted successfully:\")\n",
    "        print(comments)\n",
    "    else:\n",
    "        print(\"No comments found or an error occurred.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe0e23b-7d30-4b9b-a6d0-714fb2c89fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
